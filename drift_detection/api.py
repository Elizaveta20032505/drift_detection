"""
FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—Ä–µ–π—Ñ–∞ –æ–±—ä–µ–∫—Ç–æ–≤.
"""
from fastapi import FastAPI, File, UploadFile, HTTPException, Form, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import cv2
import numpy as np
import os
import tempfile
import shutil
from pathlib import Path
import time
import pickle
import zipfile
import uuid
import threading
from collections import deque
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
from drift_detector import ObjectDriftDetector
from cvat_loader import extract_images_from_archive

DATA_DIR = os.path.join(os.getcwd(), "data")
os.makedirs(DATA_DIR, exist_ok=True)
MODEL_PATH_FILE = os.path.join(DATA_DIR, "model_path.txt")
MODEL_WEIGHTS_PATH = os.path.join(DATA_DIR, "trained_model.pt")
DATASET_PATH_FILE = os.path.join(DATA_DIR, "dataset_path.txt")
DATASET_CONFIG_PATH = os.path.join(DATA_DIR, "dataset_config.yaml")
BASELINE_IMAGES_FILE = os.path.join(DATA_DIR, "baseline_images.pkl")
TRAINING_STATUS_FILE = os.path.join(DATA_DIR, "training_status.txt")
TRAINING_ERROR_FILE = os.path.join(DATA_DIR, "training_error.txt")

print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {DATA_DIR}")
print(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏: {MODEL_PATH_FILE}")
print(f"–§–∞–π–ª –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏: {MODEL_WEIGHTS_PATH}")
print(f"–§–∞–π–ª —Å—Ç–∞—Ç—É—Å–∞ –æ–±—É—á–µ–Ω–∏—è: {TRAINING_STATUS_FILE}")
print(f"–§–∞–π–ª –æ—à–∏–±–∫–∏ –æ–±—É—á–µ–Ω–∏—è: {TRAINING_ERROR_FILE}")

app = FastAPI(
    title="Drift Detection API",
    description="API –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –¥—Ä–µ–π—Ñ–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Å–∏—Å—Ç–µ–º–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

drift_detector: Optional[ObjectDriftDetector] = None
trained_model_path: Optional[str] = None
baseline_dataset_path: Optional[str] = None
baseline_images: List[np.ndarray] = []
baseline_ready: bool = False
training_status: str = "not_started"
training_error: Optional[str] = None

_pretrained_detector: Optional[ObjectDriftDetector] = None
_pretrained_object_classes: Optional[List[str]] = None


def get_pretrained_detector(object_classes: Optional[List[str]] = None) -> ObjectDriftDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –Ω–∞ YOLO11l (–∫–∞—á–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ), –±–µ–∑ SAM. object_classes ‚Äî —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–ª–∞—Å—Å–∞–º COCO (person, car, ...)."""
    global _pretrained_detector, _pretrained_object_classes
    if object_classes is None:
        object_classes = []
    same = _pretrained_object_classes == object_classes if _pretrained_object_classes is not None else not object_classes
    if _pretrained_detector is not None and same:
        return _pretrained_detector
    dummy = [np.zeros((64, 64, 3), dtype=np.uint8)]
    _pretrained_detector = ObjectDriftDetector(
        baseline_images=dummy,
        yolo_model_path=None,
        allowed_class_ids=None,
        allowed_name_tokens=object_classes if object_classes else None,
        use_sam=False,
    )
    _pretrained_object_classes = object_classes[:] if object_classes else []
    return _pretrained_detector

# –ú–µ—Ç—Ä–∏–∫–∏ Prometheus
drift_detections = Counter('object_drift_detections_total', '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–π –¥—Ä–µ–π—Ñ–∞')
drift_psi = Histogram('object_drift_psi', 'PSI –º–µ—Ç—Ä–∏–∫–∞ –¥—Ä–µ–π—Ñ–∞')
drift_kl_divergence = Histogram('object_drift_kl_divergence', 'KL divergence –º–µ—Ç—Ä–∏–∫–∞ –¥—Ä–µ–π—Ñ–∞')
drift_ks_statistic = Histogram('object_drift_ks_statistic', 'KS —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥—Ä–µ–π—Ñ–∞')
drift_ks_pvalue = Histogram('object_drift_ks_pvalue', 'KS p-value (–¥—Ä–µ–π—Ñ –ø—Ä–∏ < 0.05)')
drift_wasserstein = Histogram('object_drift_wasserstein', '–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –í–∞—Å—Å–µ—Ä—à—Ç–µ–π–Ω–∞ –ø–æ —è—Ä–∫–æ—Å—Ç–∏')
drift_js_divergence = Histogram('object_drift_js_divergence', '–î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è –î–∂–µ–Ω—Å–µ–Ω–∞-–®–µ–Ω–Ω–æ–Ω–∞')
drift_aggregate_score = Histogram('object_drift_aggregate_score', '–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –¥—Ä–µ–π—Ñ–∞ (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è)')
video_processing_seconds = Histogram('object_video_processing_seconds', '–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö')
detections_count = Gauge('object_detections_count', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–π –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ —Ç–µ–∫—É—â–µ–º –≤–∏–¥–µ–æ')
video_seconds_gauge = Gauge('object_drift_video_seconds', '–¢–µ–∫—É—â–∞—è —Å–µ–∫—É–Ω–¥–∞ –≤–∏–¥–µ–æ (–æ–¥–Ω–∞ —Å–µ—Ä–∏—è: –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –Ω–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ –≥—Ä–∞—Ñ–∏–∫ –ø–æ —Å—É—Ç–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –∑–∞–Ω–æ–≤–æ)')
drift_alert_gauge = Gauge('object_drift_alert', '–î–µ—Ç–µ–∫—Ü–∏—è –¥—Ä–µ–π—Ñ–∞ (1=–¥–∞, 0=–Ω–µ—Ç)')
ph_alert_gauge = Gauge('object_drift_ph_alert', 'Page-Hinkley –∞–ª–µ—Ä—Ç (1=–¥–∞, 0=–Ω–µ—Ç)')

VIDEO_JOBS: Dict[str, Dict[str, Any]] = {}
VIDEO_JOBS_LOCK = threading.Lock()
PROCESSED_FRAMES_DIR = os.path.join(DATA_DIR, "processed_frames")
os.makedirs(PROCESSED_FRAMES_DIR, exist_ok=True)
DRIFT_FRAME_MAX_EDGE = 320

class TrainingResponse(BaseModel):
    message: str
    status: str
    epochs: Optional[int] = None
    model_path: Optional[str] = None


class VideoJobResponse(BaseModel):
    job_id: str
    message: str

def validate_and_save_archive(archive: UploadFile) -> str:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç ZIP –∞—Ä—Ö–∏–≤"""
    if not archive.filename.lower().endswith('.zip'):
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ ZIP")

    temp_path = tempfile.mktemp(suffix='.zip')
    try:
        with open(temp_path, 'wb') as f:
            content = archive.file.read()
            f.write(content)

        with zipfile.ZipFile(temp_path, 'r') as zip_ref:
            zip_ref.testzip()

        return temp_path
    except Exception as e:
        if os.path.exists(temp_path):
            os.unlink(temp_path)
        raise HTTPException(status_code=400, detail=f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π ZIP –∞—Ä—Ö–∏–≤: {str(e)}")

def validate_and_save_video(video: UploadFile) -> str:
    """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∏–¥–µ–æ—Ñ–∞–π–ª"""
    if not video.filename.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):
        raise HTTPException(status_code=400, detail="–í–∏–¥–µ–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ MP4/AVI/MOV/MKV")

    temp_path = tempfile.mktemp(suffix=Path(video.filename).suffix)
    try:
        with open(temp_path, 'wb') as f:
            content = video.file.read()
            f.write(content)
        return temp_path
    except Exception as e:
        if os.path.exists(temp_path):
            os.unlink(temp_path)
        raise HTTPException(status_code=400, detail=f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–∏–¥–µ–æ: {str(e)}")


def apply_distortion(
    frame: np.ndarray,
    brightness: float = 0.0,
    contrast: float = 1.0,
    noise_std: float = 0.0,
    hue_shift: int = 0,
    saturation_scale: float = 1.0
) -> np.ndarray:
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∏—Å–∫–∞–∂–µ–Ω–∏—è –∫ –∫–∞–¥—Ä—É (—è—Ä–∫–æ—Å—Ç—å, –∫–æ–Ω—Ç—Ä–∞—Å—Ç, —à—É–º, —Ü–≤–µ—Ç)."""
    distorted = frame.astype(np.float32)

    if contrast != 1.0 or brightness != 0.0:
        distorted = distorted * contrast + brightness

    if noise_std > 0.0:
        noise = np.random.normal(0, noise_std, distorted.shape).astype(np.float32)
        distorted = distorted + noise

    distorted = np.clip(distorted, 0, 255).astype(np.uint8)

    if hue_shift != 0 or saturation_scale != 1.0:
        hsv = cv2.cvtColor(distorted, cv2.COLOR_BGR2HSV).astype(np.float32)
        hsv[..., 0] = (hsv[..., 0] + hue_shift) % 180
        hsv[..., 1] = np.clip(hsv[..., 1] * saturation_scale, 0, 255)
        distorted = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)

    return distorted


BRIGHT_COLORS_BGR = [
    (0, 255, 0),    # –∑–µ–ª—ë–Ω—ã–π
    (0, 0, 255),    # –∫—Ä–∞—Å–Ω—ã–π
    (255, 0, 0),    # —Å–∏–Ω–∏–π
    (0, 255, 255),  # –∂—ë–ª—Ç—ã–π
    (255, 0, 255),  # magenta
    (255, 165, 0),  # –æ—Ä–∞–Ω–∂–µ–≤—ã–π (BGR)
    (0, 191, 255),  # deep sky blue
    (203, 192, 255), # lavender
]


def draw_detections(
    frame: np.ndarray,
    detections: List[dict],
    class_colors: Optional[Dict[str, tuple]] = None,
) -> np.ndarray:
    """
    –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç –±–æ–∫—Å—ã –¥–µ—Ç–µ–∫—Ü–∏–π –Ω–∞ –∫–∞–¥—Ä–µ.
    class_colors: —Å–ª–æ–≤–∞—Ä—å class_name -> (B, G, R); –µ—Å–ª–∏ None ‚Äî –≤—Å–µ –∑–µ–ª—ë–Ω—ã–µ.
    """
    output = frame.copy()
    for det in detections:
        x1, y1, x2, y2 = det['bbox']
        conf = det.get('confidence', 0.0)
        class_name = det.get('class_name', 'object')
        label = f"{class_name} {conf:.2f}"
        color = (0, 255, 0)
        if class_colors and class_name in class_colors:
            color = class_colors[class_name]
        cv2.rectangle(output, (x1, y1), (x2, y2), color, 2)
        cv2.putText(
            output,
            label,
            (x1, max(0, y1 - 5)),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            color,
            1,
            cv2.LINE_AA,
        )
    return output


def _get_or_assign_class_colors(detections: List[dict], class_colors: Dict[str, tuple]) -> None:
    """–î–æ–ø–æ–ª–Ω—è–µ—Ç class_colors –Ω–æ–≤—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏ –∏–∑ detections (—Ü–≤–µ—Ç–∞ –∏–∑ BRIGHT_COLORS_BGR)."""
    for det in detections:
        name = det.get('class_name', 'object')
        if name not in class_colors:
            class_colors[name] = BRIGHT_COLORS_BGR[len(class_colors) % len(BRIGHT_COLORS_BGR)]

def record_drift_metrics(metrics_dict: dict, processing_time: float, job_id: Optional[str] = None, video_second: Optional[float] = None):
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ Prometheus. job_id –∏ video_second ‚Äî –¥–ª—è –≤—Ç–æ—Ä–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∫–∞–ª—ã –≤ Grafana."""
    try:
        if video_second is not None:
            video_seconds_gauge.set(video_second)
        # drift_detected –æ–∂–∏–¥–∞–µ—Ç—Å—è –∫–∞–∫ –±—É–ª–µ–≤ —Ñ–ª–∞–≥
        if metrics_dict.get('drift_detected'):
            drift_detections.inc()
            drift_alert_gauge.set(1)
        else:
            drift_alert_gauge.set(0)
        ph_alert_gauge.set(1 if metrics_dict.get('page_hinkley_alert') else 0)

        psi_value = metrics_dict.get('psi')
        if psi_value is None:
            psi_value = metrics_dict.get('psi_mean')
        if psi_value is not None:
            drift_psi.observe(float(psi_value))

        kl_value = metrics_dict.get('kl_divergence')
        if kl_value is None:
            kl_value = metrics_dict.get('kl_mean')
        if kl_value is not None:
            drift_kl_divergence.observe(float(kl_value))

        ks_value = metrics_dict.get('ks_statistic')
        if ks_value is not None:
            drift_ks_statistic.observe(float(ks_value))
        ks_pval = metrics_dict.get('ks_pvalue')
        if ks_pval is not None:
            drift_ks_pvalue.observe(float(ks_pval))

        w_value = metrics_dict.get('wasserstein_distance')
        if w_value is not None:
            drift_wasserstein.observe(float(w_value))

        js_value = metrics_dict.get('js_divergence')
        if js_value is not None:
            drift_js_divergence.observe(float(js_value))
        agg_value = metrics_dict.get('aggregate_score')
        if agg_value is not None:
            drift_aggregate_score.observe(float(agg_value))

        if processing_time is not None:
            video_processing_seconds.observe(float(processing_time))

        detections_count.set(int(metrics_dict.get('total_detections', 0)))
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –º–µ—Ç—Ä–∏–∫: {e}")


def convert_numpy_types(obj):
    if isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    if hasattr(obj, 'item'):
        return obj.item()
    return obj


def init_video_job(job_id: str, output_dir: str):
    with VIDEO_JOBS_LOCK:
        VIDEO_JOBS[job_id] = {
            "status": "running",
            "message": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–ø—É—â–µ–Ω–∞",
            "output_dir": output_dir,
            "processed_frames": 0,
            "total_frames": None,
            "metrics_history": [],
            "last_metrics": None,
            "last_detection_second": None,
            "started_at": time.time(),
            "finished_at": None,
            "error": None,
        }


def update_video_job(job_id: str, **kwargs):
    with VIDEO_JOBS_LOCK:
        if job_id in VIDEO_JOBS:
            VIDEO_JOBS[job_id].update(kwargs)


def get_video_job(job_id: str) -> Optional[Dict[str, Any]]:
    with VIDEO_JOBS_LOCK:
        return VIDEO_JOBS.get(job_id)


def process_video_job(
    job_id: str,
    video_path: str,
    loop_video: bool,
    loop_count: int,
    frame_stride: int,
    drift_window_frames: int,
    drift_window_sec: Optional[float],
    only_frames_with_detections: bool,
    distortion_mode: str,
    brightness: float,
    contrast: float,
    noise_std: float,
    hue_shift: int,
    saturation_scale: float,
    segment_duration_sec: float,
    max_duration_sec: Optional[float],
):
    output_dir = os.path.join(PROCESSED_FRAMES_DIR, job_id)
    os.makedirs(output_dir, exist_ok=True)
    init_video_job(job_id, output_dir)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        update_video_job(job_id, status="error", error="–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ")
        return

    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    update_video_job(job_id, total_frames=total_frames)
    if drift_window_sec is not None and drift_window_sec > 0:
        drift_window_frames = max(2, int(drift_window_sec * fps / max(1, frame_stride)))

    stages = [
        {"name": "original", "brightness": 0.0, "contrast": 1.0, "noise_std": 0.0, "hue_shift": 0, "saturation_scale": 1.0},
        {"name": "brightness", "brightness": brightness, "contrast": 1.0, "noise_std": 0.0, "hue_shift": 0, "saturation_scale": 1.0},
        {"name": "contrast", "brightness": 0.0, "contrast": contrast, "noise_std": 0.0, "hue_shift": 0, "saturation_scale": 1.0},
        {"name": "noise", "brightness": 0.0, "contrast": 1.0, "noise_std": noise_std, "hue_shift": 0, "saturation_scale": 1.0},
        {"name": "color", "brightness": 0.0, "contrast": 1.0, "noise_std": 0.0, "hue_shift": hue_shift, "saturation_scale": saturation_scale},
    ]

    processed_frames = 0
    global_frame_index = 0
    loops_done = 0
    start_time = time.time()
    last_detection_second = None
    frame_window = deque(maxlen=drift_window_frames)
    class_colors = {}

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                loops_done += 1
                if not loop_video or (loop_count > 0 and loops_done >= loop_count):
                    break
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue

            global_frame_index += 1
            current_second = global_frame_index / fps
            if max_duration_sec is not None and current_second >= max_duration_sec:
                break

            if frame_stride > 1 and (global_frame_index % frame_stride != 0):
                continue

            if distortion_mode == "uniform":
                processed_frame = apply_distortion(
                    frame,
                    brightness=brightness,
                    contrast=contrast,
                    noise_std=noise_std,
                    hue_shift=hue_shift,
                    saturation_scale=saturation_scale,
                )
                stage_name = "uniform"
            elif distortion_mode == "staged":
                stage_index = int(current_second / max(segment_duration_sec, 0.1)) % len(stages)
                stage = stages[stage_index]
                processed_frame = apply_distortion(
                    frame,
                    brightness=stage["brightness"],
                    contrast=stage["contrast"],
                    noise_std=stage["noise_std"],
                    hue_shift=stage["hue_shift"],
                    saturation_scale=stage["saturation_scale"],
                )
                stage_name = stage["name"]
            else:
                processed_frame = frame
                stage_name = "original"

            result = drift_detector.process_frame(processed_frame)
            detections = result['detections']
            object_images = result['object_images']

            # –í –æ–∫–Ω–æ –∫–ª–∞–¥—ë–º —É–º–µ–Ω—å—à–µ–Ω–Ω—É—é –∫–æ–ø–∏—é –∫–∞–¥—Ä–∞ –¥–ª—è –¥—Ä–µ–π—Ñ–∞ (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏, –±–µ–∑ OOM)
            h, w = processed_frame.shape[:2]
            if max(h, w) > DRIFT_FRAME_MAX_EDGE:
                scale = DRIFT_FRAME_MAX_EDGE / max(h, w)
                small_frame = cv2.resize(
                    processed_frame,
                    (max(1, int(w * scale)), max(1, int(h * scale))),
                    interpolation=cv2.INTER_AREA,
                )
            else:
                small_frame = processed_frame.copy()
            if not only_frames_with_detections or len(detections) > 0:
                frame_window.append(small_frame)
            drift_metrics_raw = None
            if len(frame_window) > 0:
                try:
                    drift_metrics_raw = drift_detector.analyzer.analyze_drift(list(frame_window))
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫ –¥—Ä–µ–π—Ñ–∞: {e}")
                    drift_metrics_raw = None

            drift_metrics = convert_numpy_types(drift_metrics_raw or {})

            if detections:
                last_detection_second = current_second
            _get_or_assign_class_colors(detections, class_colors)
            overlay = draw_detections(processed_frame, detections, class_colors)
            frame_filename = os.path.join(output_dir, f"frame_{processed_frames:06d}.jpg")
            cv2.imwrite(frame_filename, overlay)

            processed_frames += 1
            metrics_entry = {
                "frame_index": global_frame_index,
                "second": current_second,
                "processed_frames": processed_frames,
                "detections_count": len(detections),
                "drift_metrics": drift_metrics,
                "distortion_stage": stage_name,
            }

            with VIDEO_JOBS_LOCK:
                job = VIDEO_JOBS.get(job_id)
                if job is not None:
                    job["metrics_history"].append(metrics_entry)
                    job["last_metrics"] = metrics_entry
                    job["processed_frames"] = processed_frames
                    job["last_detection_second"] = last_detection_second

            if drift_metrics:
                metrics_payload = drift_metrics.copy()
                metrics_payload["total_detections"] = len(detections)
                record_drift_metrics(
                    metrics_payload,
                    time.time() - start_time,
                    job_id=job_id,
                    video_second=current_second,
                )

    except Exception as e:
        update_video_job(job_id, status="error", error=str(e))
        return
    finally:
        cap.release()
        if os.path.exists(video_path):
            os.unlink(video_path)

    update_video_job(job_id, status="completed", message="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞", finished_at=time.time())


def process_video_job_pretrained(
    job_id: str,
    video_path: str,
    object_classes: List[str],
    frame_stride: int,
    drift_window_sec: float,
    only_frames_with_detections: bool,
    loop_video: bool,
    loop_count: int,
    max_duration_sec: Optional[float],
):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π YOLO11l –±–µ–∑ baseline.
    –î—Ä–µ–π—Ñ: —Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ W —Å–µ–∫, –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å—Ç–∞—Ä—à—É—é –ø–æ–ª–æ–≤–∏–Ω—É –æ–∫–Ω–∞ —Å –º–ª–∞–¥—à–µ–π.
    –ö–∞–¥—Ä—ã —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ output_dir, —Å–∫–∞—á–∞—Ç—å: GET /video_jobs/{job_id}/download.
    """
    output_dir = os.path.join(PROCESSED_FRAMES_DIR, job_id)
    os.makedirs(output_dir, exist_ok=True)
    init_video_job(job_id, output_dir)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        update_video_job(job_id, status="error", error="–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ")
        return

    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    update_video_job(job_id, total_frames=total_frames)

    # –û–∫–Ω–æ –≤ –∫–∞–¥—Ä–∞—Ö: –∏–∑ —Å–µ–∫—É–Ω–¥ –∏ frame_stride
    frames_per_window = max(2, int(drift_window_sec * fps / max(1, frame_stride)))
    frame_window = deque(maxlen=frames_per_window)

    det = get_pretrained_detector(object_classes)
    detector, analyzer = det.detector, det.analyzer
    class_colors = {}

    processed_frames = 0
    global_frame_index = 0
    loops_done = 0
    start_time = time.time()
    last_detection_second = None

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                loops_done += 1
                if not loop_video or (loop_count > 0 and loops_done >= loop_count):
                    break
                cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                continue

            global_frame_index += 1
            current_second = global_frame_index / fps
            if max_duration_sec is not None and current_second >= max_duration_sec:
                break

            if frame_stride > 1 and (global_frame_index % frame_stride != 0):
                continue

            processed_frame = frame
            h, w = processed_frame.shape[:2]
            if max(h, w) > DRIFT_FRAME_MAX_EDGE:
                scale = DRIFT_FRAME_MAX_EDGE / max(h, w)
                small_frame = cv2.resize(processed_frame, (max(1, int(w * scale)), max(1, int(h * scale))), interpolation=cv2.INTER_AREA)
            else:
                small_frame = processed_frame.copy()

            detections = detector.detect_objects(processed_frame)
            if not only_frames_with_detections or len(detections) > 0:
                frame_window.append(small_frame)

            drift_metrics_raw = None
            if len(frame_window) >= 2:
                try:
                    drift_metrics_raw = analyzer.analyze_drift_stream(list(frame_window))
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á—ë—Ç–∞ –¥—Ä–µ–π—Ñ–∞ (stream): {e}")
            drift_metrics = convert_numpy_types(drift_metrics_raw or {})

            if detections:
                last_detection_second = current_second
            _get_or_assign_class_colors(detections, class_colors)
            overlay = draw_detections(processed_frame, detections, class_colors)
            cv2.imwrite(os.path.join(output_dir, f"frame_{processed_frames:06d}.jpg"), overlay)
            processed_frames += 1

            metrics_entry = {
                "frame_index": global_frame_index,
                "second": current_second,
                "processed_frames": processed_frames,
                "detections_count": len(detections),
                "drift_metrics": drift_metrics,
                "distortion_stage": "original",
            }
            with VIDEO_JOBS_LOCK:
                job = VIDEO_JOBS.get(job_id)
                if job:
                    job["metrics_history"].append(metrics_entry)
                    job["last_metrics"] = metrics_entry
                    job["processed_frames"] = processed_frames
                    job["last_detection_second"] = last_detection_second

            if drift_metrics:
                metrics_payload = drift_metrics.copy()
                metrics_payload["total_detections"] = len(detections)
                record_drift_metrics(metrics_payload, time.time() - start_time, job_id=job_id, video_second=current_second)
    except Exception as e:
        update_video_job(job_id, status="error", error=str(e))
        return
    finally:
        cap.release()
        if os.path.exists(video_path):
            os.unlink(video_path)
    update_video_job(job_id, status="completed", message="–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ (pretrained) –∑–∞–≤–µ—Ä—à–µ–Ω–∞", finished_at=time.time())


# API endpoints
@app.get("/status")
async def get_status():
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "model_trained": trained_model_path is not None and os.path.exists(trained_model_path),
        "trained_model_path": trained_model_path,
        "detector_ready": drift_detector is not None,
        "data_directory": DATA_DIR,
        "training_status": training_status,
        "training_error": training_error,
        "ready_for_drift_detection": drift_detector is not None and trained_model_path is not None
    }

@app.get("/metrics")
def get_metrics():
    """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ Prometheus"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


@app.post("/upload_baseline")
async def upload_baseline(
    cvat_archive: UploadFile = File(..., description="CVAT ZIP –∞—Ä—Ö–∏–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –¥–ª—è baseline")
):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç baseline-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ CVAT ZIP –∞—Ä—Ö–∏–≤–∞ –±–µ–∑ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

    –ò–∑ –∞—Ä—Ö–∏–≤–∞ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∏–≥–Ω–æ—Ä–∏—Ä—É—è —Ä–∞–∑–º–µ—Ç–∫—É), –æ–Ω–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è
    –≤ —Ñ–∞–π–ª BASELINE_IMAGES_FILE –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DriftAnalyzer.
    """
    global baseline_images, baseline_ready, drift_detector, trained_model_path

    temp_archive = validate_and_save_archive(cvat_archive)

    try:
        images = extract_images_from_archive(temp_archive)
        if not images:
            raise HTTPException(
                status_code=400,
                detail="–í –∞—Ä—Ö–∏–≤–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (jpg/png)"
            )

        baseline_images = images
        baseline_ready = True

        try:
            with open(BASELINE_IMAGES_FILE, 'wb') as f:
                pickle.dump(baseline_images, f)
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")

        if trained_model_path is not None and os.path.exists(trained_model_path):
            try:
                sam_path = "sam_b.pt" if os.path.exists("sam_b.pt") else None
                drift_detector = ObjectDriftDetector(
                    baseline_images=baseline_images,
                    yolo_model_path=trained_model_path,
                    allowed_class_ids=None,
                    sam_checkpoint_path=sam_path if sam_path else "sam_b.pt",
                    use_sam=sam_path is not None
                )
            except Exception as e:
                print(f"‚úó –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Å –Ω–æ–≤—ã–º baseline: {e}")
                raise HTTPException(
                    status_code=500,
                    detail=f"Baseline —Å–æ—Ö—Ä–∞–Ω—ë–Ω, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–µ—Ç–µ–∫—Ç–æ—Ä: {e}"
                )

        return {
            "message": f"Baseline —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω: {len(baseline_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
            "images_count": len(baseline_images),
            "model_ready": trained_model_path is not None and os.path.exists(trained_model_path),
        }
    finally:
        if os.path.exists(temp_archive):
            os.unlink(temp_archive)

@app.post("/train_model")
async def train_model(
    cvat_archive: UploadFile = File(..., description="CVAT –∞—Ä—Ö–∏–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ YOLO"),
    epochs: int = Form(default=50, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 50 –¥–ª—è —Ö–æ—Ä–æ—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞)"),
    batch_size: int = Form(default=2, description="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞"),
    imgsz: int = Form(default=320, description="–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
):
    """
    –û–±—É—á–∞–µ—Ç YOLO –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CVAT –∞—Ä—Ö–∏–≤–∞.

    Args:
        cvat_archive: CVAT –∞—Ä—Ö–∏–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ YOLO
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        imgsz: –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

    –í–ù–ò–ú–ê–ù–ò–ï: –û–±—É—á–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∏ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.
    """
    global drift_detector, trained_model_path, baseline_dataset_path, training_status, training_error

    temp_archive = validate_and_save_archive(cvat_archive)

    try:
        print("–°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ CVAT –∞—Ä—Ö–∏–≤–∞...")
        from model_trainer import prepare_dataset_from_cvat_archive
        import tempfile

        temp_dataset_dir = tempfile.mkdtemp(prefix="cvat_dataset_")
        print(f"–í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {temp_dataset_dir}")

        try:
            dataset_yaml = prepare_dataset_from_cvat_archive(
                archive_path=temp_archive,
                output_dir=temp_dataset_dir,
                object_class_id=0
            )
            baseline_dataset_path = dataset_yaml
            print(f"‚úì –î–∞—Ç–∞—Å–µ—Ç –∏–∑ CVAT —Å–æ–∑–¥–∞–Ω: {dataset_yaml}")
        except Exception as e:
            import shutil
            if os.path.exists(temp_dataset_dir):
                shutil.rmtree(temp_dataset_dir)
            raise HTTPException(
                status_code=500,
                detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ CVAT –∞—Ä—Ö–∏–≤–∞: {str(e)}"
            )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—Ä—Ö–∏–≤–∞: {str(e)}"
        )

    import threading

    training_status = "training"
    training_error = None
    with open(TRAINING_STATUS_FILE, 'w') as f:
        f.write("training")

    def train_sync():
        import shutil
        global trained_model_path, drift_detector, training_status, training_error

        data_dir = os.path.join(os.getcwd(), "data")

        try:
            print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–µ...")
            print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(baseline_images)}")
            print(f"–ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {baseline_dataset_path}")

            if epochs < 10:
                print(f"‚ö†Ô∏è  –ö–†–ò–¢–ò–ß–ù–û: {epochs} —ç–ø–æ—Ö - –≠–¢–û–ì–û –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û!")
                print("   YOLO –º–æ–¥–µ–ª–∏ –Ω—É–∂–¥–∞—é—Ç—Å—è –º–∏–Ω–∏–º—É–º –≤ 50-100 —ç–ø–æ—Ö–∞—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                print("   –° 1 —ç–ø–æ—Ö–æ–π –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Å–ª—É—á–∞–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä!")
            elif epochs < 50:
                print(f"‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: {epochs} —ç–ø–æ—Ö –º–∞–ª–æ–≤–∞—Ç–æ, —Ä–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç –ø–ª–æ—Ö–∏–º")

            # –í—ã–∑—ã–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            from model_trainer import train_yolo_model
            model_path = train_yolo_model(
                dataset_yaml=baseline_dataset_path,
                epochs=epochs,
                batch=batch_size,
                imgsz=imgsz,
                device="cpu"
            )

            print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...")

            print(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏: {model_path}")
            print(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(model_path)}")
            if os.path.exists(model_path):
                file_size = os.path.getsize(model_path)
                print(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –º–æ–¥–µ–ª–∏: {file_size} –±–∞–π—Ç")

                model_in_data = os.path.join(data_dir, "trained_model.pt")
                shutil.copy2(model_path, model_in_data)
                print(f"‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É data: {model_in_data}")
                print(f"–§–∞–π–ª –≤ data —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(model_in_data)}")

                shutil.copy2(model_path, MODEL_WEIGHTS_PATH)
                print(f"‚úì –ú–æ–¥–µ–ª—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∞ –≤ –ø–æ—Å—Ç–æ—è–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {MODEL_WEIGHTS_PATH}")
                print(f"–§–∞–π–ª –≤–µ—Å–æ–≤ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(MODEL_WEIGHTS_PATH)}")

                trained_model_path = MODEL_WEIGHTS_PATH
                with open(MODEL_PATH_FILE, 'w') as f:
                    f.write(MODEL_WEIGHTS_PATH)
                print(f"‚úì –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {MODEL_PATH_FILE}: {MODEL_WEIGHTS_PATH}")
                print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

                if len(baseline_images) > 0:
                    try:
                        with open(BASELINE_IMAGES_FILE, 'wb') as f:
                            pickle.dump(baseline_images, f)
                        print(f"‚úì Baseline —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(baseline_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {BASELINE_IMAGES_FILE}")
                    except Exception as e:
                        print(f"‚úó –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å baseline: {e}")

                try:
                    sam_path = "sam_b.pt" if os.path.exists("sam_b.pt") else None
                    drift_detector = ObjectDriftDetector(
                        baseline_images=baseline_images,
                        yolo_model_path=trained_model_path,
                        allowed_class_ids=None,
                        sam_checkpoint_path=sam_path if sam_path else "sam_b.pt",
                        use_sam=sam_path is not None
                    )
                    training_status = "completed"
                    with open(TRAINING_STATUS_FILE, 'w') as f:
                        f.write("completed")
                    print("‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–æ–≤–æ–π –º–æ–¥–µ–ª—å—é")
                except Exception as e:
                    training_status = "error"
                    training_error = f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞: {str(e)}"
                    with open(TRAINING_STATUS_FILE, 'w') as f:
                        f.write("error")
                    with open(TRAINING_ERROR_FILE, 'w') as f:
                        f.write(str(e))
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞: {e}")
            else:
                print(f"‚úó –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
                training_status = "error"
                training_error = f"–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}"
                with open(TRAINING_STATUS_FILE, 'w') as f:
                    f.write("error")
                with open(TRAINING_ERROR_FILE, 'w') as f:
                    f.write(training_error)

        except Exception as e:
            training_status = "error"
            training_error = str(e)
            with open(TRAINING_STATUS_FILE, 'w') as f:
                f.write("error")
            with open(TRAINING_ERROR_FILE, 'w') as f:
                f.write(str(e))
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            print("Traceback:")
            traceback.print_exc()

    print("–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    train_sync()

    if training_status == "completed":
        return {
            "message": "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ",
            "status": "completed",
            "epochs": epochs,
            "model_path": trained_model_path
        }
    else:
        raise HTTPException(
            status_code=500,
            detail=f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {training_error}"
        )

@app.post("/process_video", response_model=VideoJobResponse)
async def process_video(
    video: UploadFile = File(..., description="–í–∏–¥–µ–æ —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥—Ä–µ–π—Ñ–∞"),
    loop_video: bool = Form(False, description="–ó–∞—Ü–∏–∫–ª–∏–≤–∞—Ç—å –≤–∏–¥–µ–æ"),
    loop_count: int = Form(1, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–∏–∫–ª–æ–≤ (0 = –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ, –µ—Å–ª–∏ –∑–∞–¥–∞–Ω max_duration_sec)"),
    frame_stride: int = Form(5, description="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä"),
    drift_window_frames: int = Form(30, description="–†–∞–∑–º–µ—Ä —Å–∫–æ–ª—å–∑—è—â–µ–≥–æ –æ–∫–Ω–∞ –ø–æ –∫–∞–¥—Ä–∞–º –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –¥—Ä–µ–π—Ñ–∞"),
    drift_window_sec: Optional[float] = Form(None, description="–û–∫–Ω–æ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ, –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç drift_window_frames –ø–æ fps)"),
    distortion_mode: str = Form("none", description="none | uniform | staged"),
    brightness: float = Form(0.0, description="–°–º–µ—â–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏ (0-255)"),
    contrast: float = Form(1.0, description="–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ (1.0 = –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"),
    noise_std: float = Form(0.0, description="–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —à—É–º–∞"),
    hue_shift: int = Form(0, description="–°–¥–≤–∏–≥ –æ—Ç—Ç–µ–Ω–∫–∞ (0-180)"),
    saturation_scale: float = Form(1.0, description="–ú–Ω–æ–∂–∏—Ç–µ–ª—å –Ω–∞—Å—ã—â–µ–Ω–Ω–æ—Å—Ç–∏"),
    segment_duration_sec: float = Form(10.0, description="–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –ø—Ä–∏ staged-—Ä–µ–∂–∏–º–µ"),
    max_duration_sec: Optional[float] = Form(None, description="–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–µ–∫—É–Ω–¥—ã)"),
    only_frames_with_detections: bool = Form(False, description="–°—á–∏—Ç–∞—Ç—å –¥—Ä–µ–π—Ñ —Ç–æ–ª—å–∫–æ –ø–æ –∫–∞–¥—Ä–∞–º, –≥–¥–µ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –¥–µ—Ç–µ–∫—Ü–∏—è"),
):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥—Ä–µ–π—Ñ–∞.

    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ä–µ–∂–∏–º—ã:
    - none: –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π;
    - uniform: –æ–¥–Ω–æ –∏—Å–∫–∞–∂–µ–Ω–∏–µ –Ω–∞ –≤–µ—Å—å –ø–æ—Ç–æ–∫;
    - staged: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∏—Å–∫–∞–∂–µ–Ω–∏—è –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º.
    """
    global drift_detector, trained_model_path, baseline_ready

    if trained_model_path is None or not os.path.exists(trained_model_path):
        raise HTTPException(
            status_code=400,
            detail="–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ /train_model —Å CVAT –∞—Ä—Ö–∏–≤–æ–º"
        )

    if drift_detector is None:
        raise HTTPException(status_code=400, detail="–î–µ—Ç–µ–∫—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    if not baseline_ready:
        raise HTTPException(
            status_code=400,
            detail="Baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ baseline —á–µ—Ä–µ–∑ /upload_baseline"
        )

    if frame_stride < 1:
        raise HTTPException(status_code=400, detail="frame_stride –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= 1")

    if drift_window_frames < 1 and (drift_window_sec is None or drift_window_sec <= 0):
        raise HTTPException(status_code=400, detail="–ó–∞–¥–∞–π—Ç–µ drift_window_frames >= 1 –∏–ª–∏ drift_window_sec > 0")

    if distortion_mode not in {"none", "uniform", "staged"}:
        raise HTTPException(status_code=400, detail="distortion_mode –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å none|uniform|staged")

    if loop_video and loop_count == 0 and max_duration_sec is None:
        raise HTTPException(
            status_code=400,
            detail="–ü—Ä–∏ loop_count=0 —É–∫–∞–∂–∏—Ç–µ max_duration_sec, —á—Ç–æ–±—ã –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª"
        )

    temp_video = validate_and_save_video(video)
    job_id = str(uuid.uuid4())

    worker = threading.Thread(
        target=process_video_job,
        args=(
            job_id,
            temp_video,
            loop_video,
            loop_count,
            frame_stride,
            drift_window_frames,
            drift_window_sec,
            only_frames_with_detections,
            distortion_mode,
            brightness,
            contrast,
            noise_std,
            hue_shift,
            saturation_scale,
            segment_duration_sec,
            max_duration_sec,
        ),
        daemon=True,
    )
    worker.start()

    return VideoJobResponse(job_id=job_id, message="–ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –∑–∞–ø—É—â–µ–Ω–∞")


@app.post("/process_video_pretrained", response_model=VideoJobResponse)
async def process_video_pretrained(
    video: UploadFile = File(..., description="–í–∏–¥–µ–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è YOLO11l, –±–µ–∑ baseline)"),
    object_classes: str = Form("person,car", description="–ö–ª–∞—Å—Å—ã COCO —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é: person, car, truck, ..."),
    frame_stride: int = Form(5, description="–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä"),
    drift_window_sec: float = Form(10.0, description="–°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è –¥—Ä–µ–π—Ñ–∞ (—Å–µ–∫—É–Ω–¥—ã)"),
    only_frames_with_detections: bool = Form(False, description="–°—á–∏—Ç–∞—Ç—å –¥—Ä–µ–π—Ñ —Ç–æ–ª—å–∫–æ –ø–æ –∫–∞–¥—Ä–∞–º —Å –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏"),
    loop_video: bool = Form(False),
    loop_count: int = Form(1),
    max_duration_sec: Optional[float] = Form(None),
):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π YOLO11l.
    –ë–µ–∑ baseline: –¥—Ä–µ–π—Ñ –ø–æ —Å–∫–æ–ª—å–∑—è—â–µ–º—É –æ–∫–Ω—É.
    –ö–∞–¥—Ä—ã —Å —Ä–∞–∑–º–µ—Ç–∫–æ–π —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è; —Å–∫–∞—á–∞—Ç—å: GET /video_jobs/{job_id}/download.
    """
    classes = [c.strip().lower() for c in object_classes.split(",") if c.strip()]
    if not classes:
        classes = ["person", "car"]

    temp_video = validate_and_save_video(video)
    job_id = str(uuid.uuid4())
    worker = threading.Thread(
        target=process_video_job_pretrained,
        args=(
            job_id,
            temp_video,
            classes,
            frame_stride,
            drift_window_sec,
            only_frames_with_detections,
            loop_video,
            loop_count,
            max_duration_sec,
        ),
        daemon=True,
    )
    worker.start()
    return VideoJobResponse(job_id=job_id, message="–ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ (pretrained) –∑–∞–ø—É—â–µ–Ω–∞")


@app.get("/video_jobs/{job_id}")
async def get_video_job_status(job_id: str):
    job = get_video_job(job_id)
    if job is None:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    return job


@app.get("/video_jobs/{job_id}/metrics")
async def get_video_job_metrics(job_id: str, limit: int = 100):
    job = get_video_job(job_id)
    if job is None:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    metrics_history = job.get("metrics_history", [])
    return metrics_history[-limit:]


@app.get("/video_jobs/{job_id}/download")
async def download_video_job_frames(job_id: str):
    job = get_video_job(job_id)
    if job is None:
        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

    output_dir = job.get("output_dir")
    if not output_dir or not os.path.exists(output_dir):
        raise HTTPException(status_code=404, detail="–ê—Ä—Ö–∏–≤ —Å –∫–∞–¥—Ä–∞–º–∏ –ø–æ–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤")

    archive_base = os.path.join(DATA_DIR, f"processed_frames_{job_id}")
    archive_path = shutil.make_archive(archive_base, 'zip', output_dir)
    with open(archive_path, 'rb') as f:
        content = f.read()

    return Response(
        content=content,
        media_type="application/zip",
        headers={"Content-Disposition": f"attachment; filename=processed_frames_{job_id}.zip"}
    )

def load_saved_state():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global trained_model_path, baseline_dataset_path, baseline_images, drift_detector, baseline_ready

    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏–∑: {DATA_DIR}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è
    global training_status, training_error
    if os.path.exists(TRAINING_STATUS_FILE):
        try:
            with open(TRAINING_STATUS_FILE, 'r') as f:
                saved_status = f.read().strip()
                if saved_status in ["not_started", "training", "completed", "error"]:
                    training_status = saved_status
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω —Å—Ç–∞—Ç—É—Å –æ–±—É—á–µ–Ω–∏—è: {training_status}")
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")

    if os.path.exists(TRAINING_ERROR_FILE):
        try:
            with open(TRAINING_ERROR_FILE, 'r') as f:
                training_error = f.read().strip()
                if training_error:
                    print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {training_error}")
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ—à–∏–±–∫–∏ –æ–±—É—á–µ–Ω–∏—è: {e}")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –µ—Å–ª–∏ –µ—Å—Ç—å
    baseline_images = []
    baseline_ready = False
    if os.path.exists(BASELINE_IMAGES_FILE):
        try:
            with open(BASELINE_IMAGES_FILE, 'rb') as f:
                baseline_images = pickle.load(f)
            if isinstance(baseline_images, list) and len(baseline_images) > 0:
                baseline_ready = True
                print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω baseline: {len(baseline_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ {BASELINE_IMAGES_FILE}")
            else:
                baseline_images = []
                baseline_ready = False
                print(f"‚úó –§–∞–π–ª baseline –ø—É—Å—Ç–æ–π, baseline –±—É–¥–µ—Ç —Å—á–∏—Ç–∞—Ç—å—Å—è –Ω–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º")
        except Exception as e:
            baseline_images = []
            baseline_ready = False
            print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ baseline –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
    print(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å –≤: {MODEL_WEIGHTS_PATH}")
    print(f"–§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(MODEL_WEIGHTS_PATH)}")
    if os.path.exists(MODEL_WEIGHTS_PATH):
        trained_model_path = MODEL_WEIGHTS_PATH
        print(f"‚úì –ù–∞–π–¥–µ–Ω–∞ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {MODEL_WEIGHTS_PATH}")
        print(f"–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(MODEL_WEIGHTS_PATH) if os.path.exists(MODEL_WEIGHTS_PATH) else 'N/A'} –±–∞–π—Ç")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {DATA_DIR}:")
        try:
            files = os.listdir(DATA_DIR)
            for f in files:
                print(f"  - {f}")
        except Exception as e:
            print(f"  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å baseline
        try:
            if baseline_ready:
                sam_path = "sam_b.pt" if os.path.exists("sam_b.pt") else None
                drift_detector = ObjectDriftDetector(
                    baseline_images=baseline_images,
                    yolo_model_path=trained_model_path,
                    allowed_class_ids=None,
                    sam_checkpoint_path=sam_path if sam_path else "sam_b.pt",
                    use_sam=sam_path is not None
                )
                print(f"‚úì –î–µ—Ç–µ–∫—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é –∏ baseline ({len(baseline_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
            else:
                drift_detector = None
                print("‚úó Baseline –Ω–µ –Ω–∞–π–¥–µ–Ω, –¥–µ—Ç–µ–∫—Ç–æ—Ä –¥—Ä–µ–π—Ñ–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–æ–∂–∏–¥–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∫–∞ baseline)")
        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞: {e}")
    else:
        print("‚úó –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
load_saved_state()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
